Monolithic applications    ---->    If all the functionalities of a project exist in a single codebase, then that application is known as a monolithic application.

We design our application in various layers like presentation, service, and persistence and then deploy that codebase as a single jar/war file. This is nothing but a monolithic application, where “mono” represents the single codebase containing all the required functionalities. 
---------------------------------------------------------------------------------------------------
Microservices ---->  It is an architectural development style in which the application is made up of smaller services that handle a small portion of the functionality and data by communicating with each other directly using lightweight protocols like HTTP.
---------------------------------------------------------------------------------------------------------------------------------

Docker is a platform and toolset designed to simplify the creation, deployment, and management of applications using containerization technology. Containerization allows you to package an application and its dependencies together into a single unit called a "container." These containers are isolated environments that ensure an application runs consistently and reliably across different computing environments, whether it's a developer's laptop, a test server, or a production data center.

Here are some key aspects and components of Docker:

----------->   Containerization: Containers are lightweight, standalone, and executable packages that contain everything needed to run an application, including the code, runtime, system tools, libraries, and settings. Containers are based on a shared operating system kernel and are isolated from each other, making them highly efficient and portable.

----------->   Docker Engine: Docker relies on the Docker Engine, which is the core component responsible for creating and running containers. The Docker Engine includes the Docker daemon, a REST API for interacting with Docker, and a command-line interface (CLI) for managing containers and images.

----------->   Images: Docker images are read-only templates that define the contents and configuration of a container. Images are used as the basis for creating containers. They can be stored in registries (like Docker Hub) and shared among developers and environments.

----------->   Dockerfile: To create a Docker image, you typically define its contents and configuration using a Dockerfile. A Dockerfile is a simple text file that specifies the base image, additional layers, and instructions for setting up the container environment.

----------->   Docker Compose: Docker Compose is a tool for defining and running multi-container applications. It allows you to define a set of interconnected containers and their configurations in a single YAML file, making it easier to manage complex application stacks.

----------->   Docker Swarm and Kubernetes: Docker Swarm and Kubernetes are orchestration tools that help you manage and scale containers in a production environment. They automate tasks like load balancing, scaling, and recovery, making it easier to deploy and maintain containerized applications at scale.

----------->   Portability: Containers are highly portable, making it possible to move applications seamlessly between different environments, from development to testing to production. This consistency reduces "it works on my machine" issues and improves collaboration.

----------->   Isolation: Containers provide a level of isolation that ensures applications don't interfere with each other. This isolation allows you to run multiple containers on a single host while maintaining security and performance.

----------->   Resource Efficiency: Containers are more resource-efficient than traditional virtual machines (VMs) because they share the same operating system kernel. This leads to faster startup times and lower overhead.

----------->   Security: Docker provides security features like namespace isolation, control groups (cgroups), and capabilities management to enhance the security of containerized applications. Additionally, Docker offers tools for scanning images for vulnerabilities.
----------------------------------------------------------------------------------------------------
What is DevOps, and why is it important?

DevOps is a cultural and technical movement in the field of software development and IT operations that aims to improve collaboration, communication, and integration between development (Dev) and operations (Ops) teams. It seeks to break down the traditional silos between these two functions and create a more streamlined and efficient software development lifecycle.

DevOps emphasizes automation, continuous integration and continuous deployment (CI/CD), and a shared responsibility for the entire application lifecycle, from code development to deployment and beyond.

Here's why DevOps is important---->
Faster Software Delivery
Improved Collaboration:
Enhanced Quality
Reduced Risk
Efficient Resource Utilization
Innovation
Feedback Loop
Scalability:
Flexibility
--------------------------------------------------------------------------------------------------
Image:

An image is a lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, libraries, and system tools.

Docker images serve as a blueprint or template for creating containers. They are read-only, meaning that once an image is created, it cannot be modified. To make changes, you create a new image.
--------------------------------------------------------------------------------------------------
Container:

A container is a running instance of a Docker image. It is a lightweight, isolated environment that encapsulates an application and its dependencies.

Containers are launched from Docker images and are completely isolated from the host system and other containers. They have their own filesystem, network, and process space.

Containers are runnable and executable. You can start, stop, pause, restart, and remove containers.

INSTALL:
yum install docker -y
service start docker
service status docker


COMMANDS:
docker images: to show a list of images
docker pull ubuntu: to get ubuntu image
docker run ubuntu: to create a container
docker run -it --name cont2 ubuntu: it will create cont2 with interactive mode
docker ps -a: to show list of containers
docker commit c5589fa2f54a swiggy : to copy the image data into the other image.

OS LEVEL OF VIRTUALIZATION:
cat /etc/os-release
apt update -y
apt upgrade -y

apt = package manager = advance pacakger tool

touch file{1..5}
apt install apache2 maven -y
ctrl p q: to exit from container

----------------------------------------------------------------------------------------------------------------------------

Virtualization is a technology that allows multiple virtual instances or environments to run on a single physical hardware system, often referred to as a host or host machine. These virtual instances are called virtual machines (VMs) or containers, and they behave as if they are independent and dedicated systems, even though they share underlying hardware resources.

Virtualization provides several benefits, including:

Resource Consolidation: By running multiple VMs on a single physical server, you can make more efficient use of your hardware resources, such as CPU, memory, storage, and network bandwidth. This consolidation can reduce hardware costs and improve resource utilization.

Isolation: Virtualization provides strong isolation between VMs. Each VM runs in its own isolated environment, which means that software running in one VM cannot directly affect or interfere with software running in another VM. This isolation enhances security and stability.

Server Consolidation: Virtualization is commonly used in data centers to consolidate multiple physical servers into a smaller number of physical machines. This reduces the physical footprint, power consumption, and cooling requirements.

Flexibility and Scalability: Virtual machines can be easily provisioned, cloned, and scaled up or down as needed. This flexibility allows for rapid deployment of new services or applications and makes it easier to adjust resources to meet changing demands.

Hardware Independence: VMs are not tied to specific physical hardware. This means you can migrate VMs between different physical servers without significant downtime, making maintenance and disaster recovery more manageable.

Resource Management: Virtualization platforms often provide tools for resource management and allocation. You can allocate specific amounts of CPU, memory, and storage to each VM, ensuring that critical applications have the resources they need.

There are two primary types of virtualization:

Hardware Virtualization (Hypervisor-Based): In this approach, a hypervisor is a thin layer of software that runs directly on the physical hardware and manages the virtual machines. There are two types of hypervisors:

Type 1 Hypervisor (Bare-Metal): This hypervisor runs directly on the hardware without the need for a host operating system. Examples include VMware vSphere/ESXi, Microsoft Hyper-V, and Xen.
Type 2 Hypervisor (Hosted): This hypervisor runs on top of a host operating system and is often used for development and testing. Examples include VMware Workstation, Oracle VirtualBox, and Parallels Desktop.
Container Virtualization: Containers provide a lightweight form of virtualization where applications and their dependencies are packaged together in a container image. These containers share the host operating system's kernel and run as isolated processes. Containerization platforms like Docker and Kubernetes have gained popularity for their efficiency and portability.
----------------------------------------------------------------------------------------------------------------
DOCKER FILE:
It is an automated way to create images.
it will have components, which are in capital letters.
in Dockerfile D must be capital.

A Dockerfile is a text file used to define the configuration and instructions required to build a Docker image. Docker images are the building blocks of containers, and a Dockerfile contains a set of commands that specify how to create an image. 

BASIC CODE OF DOCKERFILE------------>
# Comment lines start with a "#" symbol

# Use a base image as the starting point
----------->FROM base-image:tag

# Set the working directory inside the container
----------->WORKDIR /app

# Copy files from the host machine into the container
----------->COPY ./source /app/destination

# Install dependencies (e.g., using package managers)
----------->RUN apt-get update && apt-get install -y package-name

# Set environment variables
----------->ENV KEY=value

# Expose a port on which the container will listen
----------->EXPOSE 80

# Define a command to run when the container starts
----------->CMD ["executable", "arg1", "arg2"]

Common Instructions:

1.FROM: Specifies the base image from which to build the new image. All subsequent instructions are based on this image.

2.WORKDIR: Sets the working directory for subsequent instructions. It's where commands like RUN, COPY, and CMD will be executed.

3.COPY: Copies files and directories from the host machine into the image's filesystem.

4.RUN: Executes commands within the image during the build process. These commands can install packages, configure settings, and perform various tasks.

5.ENV: Sets environment variables within the image. These variables can be used by applications and scripts running in the container.

6.EXPOSE: Specifies which ports the container will listen on when running. It doesn't actually publish the ports; that's done at runtime using the -p option.

7.CMD: Defines the default command to run when a container is started from the image. There can only be one CMD instruction in a Dockerfile, but it can be overridden at runtime.

8.ADD: copies the internet file to container (or) extract the files

9.ENTRYPOINT: Similar to CMD, but allows you to configure a container's entry point as an executable. It's often used for containers running as applications.

10.VOLUME: Creates a mount point with the specified name in the container. It's often used to persist data outside the container filesystem.
----------------------------------------------------------------------------------------------------------------------
DEPLOYMENT:

Dockerfile
FROM ubuntu
RUN apt-get update -y
RUN apt-get install apache2 -y
COPY index.html /var/www/html/
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]
------------------------------------------------------------------------------------------------------------------

DOCKER VOLUMES:-----> Creates a mount point with the specified name in the container. It's often used to persist data outside the container filesystem.

1.When we create a Container then Volume will be created.
2.Volume is simply a directory inside our container.
3.First, we have to declare the directory Volume and then share Volume.
4.Even if we stop the container still, we can access the volume.
5.You can declare directory as a volume only while creating container.
6.We can’t create volume from existing container.
7.You can share one volume across many number of Containers.
8.Volume will not be included when you update an image.
9.If Container-1 volume is shared to Container-2 the changes made by Container-2 will be also available in the Container-1.
----------------------------------------------------------------------------------------------------------------------DOCKER COMPOSE:

It is a tool used to build, run and ship multiple containers for application.
It is used to create multiple containers in a single host.
It used the YAML file to manage multi containers as a single service.
The Compose file provides a way to document and configure all of the application’s service dependencies (databases, queues, caches, web service APIs, etc). 

COMPOSE/DOCKER COMPOSE FILE: (yml=yaml)
We provide details about services.
ex: image, port, volume ----

INSTALLATION:

sudo curl -L "https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
ls /usr/local/bin/
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
docker-compose version

WORKING:

version: '3'
services:
  movies:
    image: paymovies:v1
    ports:
      - "90:80"
  train:
    image: paytrain:v1
    ports:
      - "91:80"
  dth:
    image: paydth:v1
    ports:
      - "92:80"
  recharge:
    image: payrecharge:v1
    ports:
      - "93:80"
-----------------------------------------------------------------------------------------------------------------
Docker Swarm ------> 

Docker swarm is an orchestration service within docker that allows us to manage and handle multiple containers at the same time.
It is a group of servers that runs the docker application.
It is used to manage the containers on multiple servers.
This can be implemented by the cluster.
The activities of the cluster are controlled by a swarm manager, and machines that have joined the cluster is called swarm worker.

Docker Engine helps to create Docker Swarm.
There are mainly worker nodes and manager nodes.
The worker nodes are connected to the manager nodes.
So any scaling or update needs to be done first it will go to the manager node.
From the manager node, all the things will go to the worker node.
Manager nodes are used to divide the work among the worker nodes. 
Each worker node will work on an individual service for better performance.

 
To create a service: docker service create —name devops —replicas 2 image_name

Note: image should be present on all the servers

To update the image service: docker service update —image image_name service_name

Note: we can change image,

To rollback the service: docker service rollback service_name

To scale: docker service scale service_name=3

To check the history: docker service logs

To check the containers: docker service ps service_name

To inspect: docker service inspect service_name

To remove: docker service rm service_name




docker swarm init --advertise-addr 172.31.27.75
    2  docker node ls
    3  docker ps -a
    4  docker run -itd --name cont1 ubuntu 
    5  vim Dockerfile
    6  vim index.html
    7  docker build -t img:v1 .
    8  docker images
    9  docker tag img:v1 rahamshaik/rahamnewapp
   10  docker images
   11  docker push rahamshaik/rahamnewapp
   12  docker login 
   13  docker push rahamshaik/rahamnewapp
   14  docker service create --name raham --replicas 6 --publish 80:80 rahamshaik/rahamnewapp:latest
   15  docker ps -a
   16  docker service ls
   17  docker service raham ps 
   18  docker service ps raham
   19  docker service ls
   20  vim index.html 
   21  docker build -t image:v2 .
   22  docker images
   23  docker tag img:v2 rahamshaik/rahamnewapp
   24  docker tag image:v2 rahamshaik/rahamnewapp
   25  docker images
   26  docker push rahamshaik/rahamnewapp
   27  docker service create --name raham2 --replicas 5 --publish 81:80 rahamshaik/rahamnewapp:latest
   28  docker service ls
   29  docker service raham2 scale=10
   30  docker service scale raham2=10
   31  docker service ls
   32  docker service scale raham2=20
   33  docker service scale raham2=10
   34  docker service ps raham2
   35  docker service logs raham2
   36  docker node ls
   37  docker node rm joe5t7e970tntz2phruzoeukc
   38  docker node ls
   39  docker ps
   40  docker top cb2f4997df2d
   41  docker ps -a
   42  docker stop $(docker ps -a -q)
   43  docker ps -a
   44  docker service rm raham
   45  docker service rm raham2
   46  docker ps -a
   47  docker run -it --name jenkins -p 8080:8080 jenkins/jenkins:lts
   48  docker pull tomcat 
   49  yum install docker -y
   50  systemctl start docker
   51  systemctl status docker
   52  hostnamectl set-hostame manager
   53  hostnamectl set-hostname manager
   54  sudo -i
